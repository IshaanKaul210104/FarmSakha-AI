{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4fd5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/Fertilizer_Prediction_Remarks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f72c0863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Temperature', 'Moisture', 'Rainfall', 'PH', 'Nitrogen', 'Phosphorous', 'Potassium', 'Carbon', 'Soil', 'Crop', 'Fertilizer', 'Remark']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cbe338c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature    0\n",
      "Moisture       0\n",
      "Rainfall       0\n",
      "PH             0\n",
      "Nitrogen       0\n",
      "Phosphorous    0\n",
      "Potassium      0\n",
      "Carbon         0\n",
      "Soil           0\n",
      "Crop           0\n",
      "Fertilizer     0\n",
      "Remark         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02c76be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remark\n",
      "Rich in phosphorus, essential for root development. Prefer this for phosphorus-deficient soils to improve plant establishment.                             1054\n",
      "Improves water retention in dry soils. Prefer this for soils with low moisture to prevent water stress in plants.                                           675\n",
      "Enhances organic matter and improves soil structure. Prefer this for low-carbon soils to boost soil health naturally.                                       375\n",
      "High potassium content, improves fruit and flower quality. Prefer this for potassium-deficient soils to enhance crop productivity.                          326\n",
      "Neutralizes acidic soil and improves pH balance. Prefer this to correct low soil pH, improving nutrient availability.                                       181\n",
      "Provides a balanced mix of nitrogen, phosphorus, and potassium for loamy soils. Prefer this for general-purpose fertilization in well-structured soils.     157\n",
      "Provides high nitrogen, ideal for rapid leafy growth. Prefer this for nitrogen-deficient soils as it supports vegetative growth.                            154\n",
      "Enhances fertility naturally, ideal for peaty soils. Prefer this to improve soil fertility while maintaining organic farming practices.                      95\n",
      "Corrects alkaline soil, adds calcium and sulfur. Prefer this to lower soil pH and enhance soil structure.                                                    52\n",
      "Suitable for general use across various soil and crop types. Prefer this when no specific deficiency is detected.                                            31\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Remark'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f35dce33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertilizer\n",
      "DAP                           1054\n",
      "Water Retaining Fertilizer     675\n",
      "Compost                        375\n",
      "Muriate of Potash              326\n",
      "Lime                           181\n",
      "Balanced NPK Fertilizer        157\n",
      "Urea                           154\n",
      "Organic Fertilizer              95\n",
      "Gypsum                          52\n",
      "General Purpose Fertilizer      31\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Fertilizer'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e60ddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation Accuracy (XGBoost): 0.9839\n",
      "\n",
      "Classification Report (XGBoost):\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "   Balanced NPK Fertilizer       0.97      1.00      0.98        31\n",
      "                   Compost       1.00      1.00      1.00        75\n",
      "                       DAP       1.00      0.98      0.99       211\n",
      "General Purpose Fertilizer       1.00      1.00      1.00         6\n",
      "                    Gypsum       1.00      0.82      0.90        11\n",
      "                      Lime       0.95      1.00      0.97        36\n",
      "         Muriate of Potash       1.00      0.98      0.99        65\n",
      "        Organic Fertilizer       0.95      1.00      0.97        19\n",
      "                      Urea       0.91      0.97      0.94        31\n",
      "Water Retaining Fertilizer       0.99      0.99      0.99       135\n",
      "\n",
      "                  accuracy                           0.98       620\n",
      "                 macro avg       0.98      0.97      0.97       620\n",
      "              weighted avg       0.98      0.98      0.98       620\n",
      "\n",
      "✅ Training Accuracy (XGBoost): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# 1. Imports\n",
    "# =====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# =====================\n",
    "# 2. Load Dataset\n",
    "# =====================\n",
    "# Assuming your dataframe is named df\n",
    "# Drop 'Remark' since it's not used for training\n",
    "X = df.drop([\"Remark\", \"Fertilizer\"], axis=1)\n",
    "y = df[\"Fertilizer\"]\n",
    "\n",
    "# =====================\n",
    "# 3. Encode Categorical Features\n",
    "# =====================\n",
    "label_encoders = {}\n",
    "for col in [\"Soil\", \"Crop\"]:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode target labels (fertilizer names)\n",
    "le_target = LabelEncoder()\n",
    "y = le_target.fit_transform(y)\n",
    "\n",
    "# =====================\n",
    "# 4. Scale Numerical Features\n",
    "# =====================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# =====================\n",
    "# 5. Train-Test Split\n",
    "# =====================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# =====================\n",
    "# 6. Train XGBoost Model\n",
    "# =====================\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=len(le_target.classes_),\n",
    "    eval_metric=\"mlogloss\"\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# =====================\n",
    "# 7. Evaluation\n",
    "# =====================\n",
    "y_pred = xgb.predict(X_val)\n",
    "\n",
    "val_acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"✅ Validation Accuracy (XGBoost): {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (XGBoost):\")\n",
    "print(classification_report(y_val, y_pred, target_names=le_target.classes_))\n",
    "\n",
    "# Training accuracy\n",
    "train_pred = xgb.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "print(f\"✅ Training Accuracy (XGBoost): {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51119695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as 'fertilizer_xgb_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "joblib.dump(xgb, \"fertilizer_xgb_model.pkl\")\n",
    "print(\"✅ Model saved as 'fertilizer_xgb_model.pkl'\")\n",
    "\n",
    "# Later, to load the model:\n",
    "# loaded_model = joblib.load(\"fertilizer_xgb_model.pkl\")\n",
    "# y_pred_loaded = loaded_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab4d361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation Accuracy: 0.9838709677419355\n",
      "\n",
      "Classification Report (XGBoost on Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Adzuki Beans       0.97      1.00      0.98        31\n",
      "  Black gram       1.00      1.00      1.00        75\n",
      "    Chickpea       1.00      0.98      0.99       211\n",
      "     Coconut       1.00      1.00      1.00         6\n",
      "      Coffee       1.00      0.82      0.90        11\n",
      "      Cotton       0.95      1.00      0.97        36\n",
      "  Ground Nut       1.00      0.98      0.99        65\n",
      "        Jute       0.95      1.00      0.97        19\n",
      "Kidney Beans       0.91      0.97      0.94        31\n",
      "      Lentil       0.99      0.99      0.99       135\n",
      "\n",
      "    accuracy                           0.98       620\n",
      "   macro avg       0.98      0.97      0.97       620\n",
      "weighted avg       0.98      0.98      0.98       620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the saved XGBoost model\n",
    "xgb_loaded = joblib.load(\"fertilizer_xgb_model.pkl\")\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Get the unique labels actually in y_val\n",
    "val_labels = np.unique(y_val)\n",
    "\n",
    "print(\"✅ Validation Accuracy:\", accuracy_score(y_val, y_pred_val))\n",
    "print(\"\\nClassification Report (XGBoost on Validation Set):\")\n",
    "print(classification_report(\n",
    "    y_val,\n",
    "    y_pred_val,\n",
    "    labels=val_labels,\n",
    "    target_names=[le.classes_[i] for i in val_labels]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60adc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Keras Neural Network ===\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ishaan Kaul\\Desktop\\PROJECTS\\FarmSakha_AI\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4367 - loss: 1.6922 - val_accuracy: 0.5823 - val_loss: 1.2850\n",
      "Epoch 2/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5669 - loss: 1.2831 - val_accuracy: 0.6790 - val_loss: 1.0113\n",
      "Epoch 3/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6286 - loss: 1.0976 - val_accuracy: 0.7274 - val_loss: 0.8680\n",
      "Epoch 4/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6552 - loss: 0.9890 - val_accuracy: 0.7710 - val_loss: 0.7757\n",
      "Epoch 5/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6976 - loss: 0.8807 - val_accuracy: 0.7710 - val_loss: 0.7130\n",
      "Epoch 6/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7065 - loss: 0.8461 - val_accuracy: 0.7774 - val_loss: 0.6739\n",
      "Epoch 7/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7169 - loss: 0.8141 - val_accuracy: 0.7919 - val_loss: 0.6426\n",
      "Epoch 8/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7367 - loss: 0.7696 - val_accuracy: 0.8081 - val_loss: 0.6142\n",
      "Epoch 9/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7403 - loss: 0.7314 - val_accuracy: 0.7871 - val_loss: 0.5869\n",
      "Epoch 10/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7484 - loss: 0.7099 - val_accuracy: 0.8145 - val_loss: 0.5653\n",
      "Epoch 11/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7589 - loss: 0.6635 - val_accuracy: 0.8048 - val_loss: 0.5450\n",
      "Epoch 12/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.6482 - val_accuracy: 0.8177 - val_loss: 0.5163\n",
      "Epoch 13/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7843 - loss: 0.6214 - val_accuracy: 0.8242 - val_loss: 0.5037\n",
      "Epoch 14/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7782 - loss: 0.6171 - val_accuracy: 0.8435 - val_loss: 0.4826\n",
      "Epoch 15/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7754 - loss: 0.6081 - val_accuracy: 0.8419 - val_loss: 0.4720\n",
      "Epoch 16/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7944 - loss: 0.5758 - val_accuracy: 0.8323 - val_loss: 0.4635\n",
      "Epoch 17/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8073 - loss: 0.5526 - val_accuracy: 0.8452 - val_loss: 0.4415\n",
      "Epoch 18/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8069 - loss: 0.5373 - val_accuracy: 0.8323 - val_loss: 0.4418\n",
      "Epoch 19/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8141 - loss: 0.5434 - val_accuracy: 0.8532 - val_loss: 0.4262\n",
      "Epoch 20/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8185 - loss: 0.5248 - val_accuracy: 0.8500 - val_loss: 0.4206\n",
      "✅ Keras Validation Accuracy: 0.8500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\n",
      "Classification Report (Keras NN):\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "   Balanced NPK Fertilizer       0.73      0.77      0.75        31\n",
      "                   Compost       0.86      0.89      0.88        75\n",
      "                       DAP       0.90      0.97      0.94       211\n",
      "General Purpose Fertilizer       0.60      0.50      0.55         6\n",
      "                    Gypsum       0.75      0.55      0.63        11\n",
      "                      Lime       0.86      0.53      0.66        36\n",
      "         Muriate of Potash       0.72      0.74      0.73        65\n",
      "        Organic Fertilizer       0.67      0.74      0.70        19\n",
      "                      Urea       0.85      0.71      0.77        31\n",
      "Water Retaining Fertilizer       0.89      0.88      0.89       135\n",
      "\n",
      "                  accuracy                           0.85       620\n",
      "                 macro avg       0.78      0.73      0.75       620\n",
      "              weighted avg       0.85      0.85      0.85       620\n",
      "\n",
      "\n",
      "=== XGBoost Classifier ===\n",
      "✅ XGBoost Accuracy: 0.9838709677419355\n",
      "\n",
      "Classification Report (XGBoost):\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "   Balanced NPK Fertilizer       0.97      1.00      0.98        31\n",
      "                   Compost       1.00      1.00      1.00        75\n",
      "                       DAP       1.00      0.98      0.99       211\n",
      "General Purpose Fertilizer       1.00      1.00      1.00         6\n",
      "                    Gypsum       1.00      0.82      0.90        11\n",
      "                      Lime       0.95      1.00      0.97        36\n",
      "         Muriate of Potash       1.00      0.98      0.99        65\n",
      "        Organic Fertilizer       0.95      1.00      0.97        19\n",
      "                      Urea       0.91      0.97      0.94        31\n",
      "Water Retaining Fertilizer       0.99      0.99      0.99       135\n",
      "\n",
      "                  accuracy                           0.98       620\n",
      "                 macro avg       0.98      0.97      0.97       620\n",
      "              weighted avg       0.98      0.98      0.98       620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # =====================\n",
    "# # 1. Imports\n",
    "# # =====================\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # =====================\n",
    "# # 2. Load + Preprocess\n",
    "# # =====================\n",
    "# # Assuming your DataFrame is called df\n",
    "# # Drop \"Remark\" since it's not needed\n",
    "# X = df.drop([\"Fertilizer\", \"Remark\"], axis=1)\n",
    "# y = df[\"Fertilizer\"]\n",
    "\n",
    "# # Encode categorical columns (Soil, Crop)\n",
    "# for col in [\"Soil\", \"Crop\"]:\n",
    "#     if X[col].dtype == \"object\":  # only encode if categorical\n",
    "#         X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "# # Encode target (Fertilizer)\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(y)\n",
    "\n",
    "# # Scale numeric features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Train/Validation Split\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# # =====================\n",
    "# # 3. Keras Neural Network\n",
    "# # =====================\n",
    "# print(\"\\n=== Keras Neural Network ===\")\n",
    "# model = keras.Sequential([\n",
    "#     layers.Dense(128, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Dense(64, activation=\"relu\"),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Dense(len(le.classes_), activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=20, batch_size=32, verbose=1\n",
    "# )\n",
    "\n",
    "# val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "# print(f\"✅ Keras Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# # Predictions\n",
    "# y_pred_nn = np.argmax(model.predict(X_val), axis=1)\n",
    "# print(\"\\nClassification Report (Keras NN):\")\n",
    "# print(classification_report(y_val, y_pred_nn, target_names=le.classes_))\n",
    "\n",
    "# # =====================\n",
    "# # 4. XGBoost Classifier\n",
    "# # =====================\n",
    "# print(\"\\n=== XGBoost Classifier ===\")\n",
    "# xgb = XGBClassifier(\n",
    "#     n_estimators=500,\n",
    "#     learning_rate=0.05,\n",
    "#     max_depth=6,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=42,\n",
    "#     objective=\"multi:softmax\",\n",
    "#     num_class=len(le.classes_),\n",
    "#     eval_metric=\"mlogloss\"\n",
    "# )\n",
    "\n",
    "# # Ensure integer labels\n",
    "# y_train = np.array(y_train, dtype=np.int32)\n",
    "# y_val = np.array(y_val, dtype=np.int32)\n",
    "\n",
    "# # Train XGBoost\n",
    "# xgb.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# y_pred_xgb = xgb.predict(X_val)\n",
    "\n",
    "# print(\"✅ XGBoost Accuracy:\", accuracy_score(y_val, y_pred_xgb))\n",
    "# print(\"\\nClassification Report (XGBoost):\")\n",
    "# print(classification_report(y_val, y_pred_xgb, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9745a57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Keras Neural Network ===\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ishaan Kaul\\Desktop\\PROJECTS\\FarmSakha_AI\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.2025 - loss: 1.9114 - val_accuracy: 0.4500 - val_loss: 1.8405\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1899 - loss: 1.8842 - val_accuracy: 0.5500 - val_loss: 1.7681\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1899 - loss: 1.8313 - val_accuracy: 0.5500 - val_loss: 1.6968\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3544 - loss: 1.7505 - val_accuracy: 0.6000 - val_loss: 1.6308\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4557 - loss: 1.6595 - val_accuracy: 0.6000 - val_loss: 1.5652\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4937 - loss: 1.6630 - val_accuracy: 0.6500 - val_loss: 1.5043\n",
      "Epoch 7/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.4937 - loss: 1.5651 - val_accuracy: 0.6500 - val_loss: 1.4488\n",
      "Epoch 8/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5696 - loss: 1.4355 - val_accuracy: 0.6000 - val_loss: 1.3933\n",
      "Epoch 9/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6076 - loss: 1.4094 - val_accuracy: 0.6000 - val_loss: 1.3375\n",
      "Epoch 10/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5696 - loss: 1.3754 - val_accuracy: 0.6000 - val_loss: 1.2822\n",
      "Epoch 11/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5696 - loss: 1.3348 - val_accuracy: 0.6000 - val_loss: 1.2288\n",
      "Epoch 12/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5570 - loss: 1.3014 - val_accuracy: 0.6500 - val_loss: 1.1793\n",
      "Epoch 13/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5696 - loss: 1.3109 - val_accuracy: 0.6000 - val_loss: 1.1334\n",
      "Epoch 14/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5570 - loss: 1.2382 - val_accuracy: 0.6000 - val_loss: 1.0892\n",
      "Epoch 15/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5949 - loss: 1.1727 - val_accuracy: 0.6500 - val_loss: 1.0472\n",
      "Epoch 16/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5823 - loss: 1.1334 - val_accuracy: 0.6500 - val_loss: 1.0085\n",
      "Epoch 17/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6709 - loss: 1.1027 - val_accuracy: 0.7500 - val_loss: 0.9697\n",
      "Epoch 18/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6835 - loss: 1.0405 - val_accuracy: 0.7500 - val_loss: 0.9328\n",
      "Epoch 19/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6582 - loss: 1.0013 - val_accuracy: 0.8000 - val_loss: 0.8960\n",
      "Epoch 20/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6962 - loss: 0.9919 - val_accuracy: 0.8000 - val_loss: 0.8595\n",
      "✅ Keras Validation Accuracy: 0.8000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\n",
      "Classification Report (Keras NN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    10-26-26       0.50      1.00      0.67         1\n",
      "    14-35-14       1.00      1.00      1.00         3\n",
      "    17-17-17       0.00      0.00      0.00         1\n",
      "       20-20       1.00      0.67      0.80         3\n",
      "       28-28       1.00      0.33      0.50         3\n",
      "         DAP       1.00      1.00      1.00         4\n",
      "        Urea       0.62      1.00      0.77         5\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.73      0.71      0.68        20\n",
      "weighted avg       0.83      0.80      0.77        20\n",
      "\n",
      "\n",
      "=== XGBoost Classifier ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ishaan Kaul\\Desktop\\PROJECTS\\FarmSakha_AI\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Ishaan Kaul\\Desktop\\PROJECTS\\FarmSakha_AI\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Ishaan Kaul\\Desktop\\PROJECTS\\FarmSakha_AI\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost Accuracy: 1.0\n",
      "\n",
      "Classification Report (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    10-26-26       1.00      1.00      1.00         1\n",
      "    14-35-14       1.00      1.00      1.00         3\n",
      "    17-17-17       1.00      1.00      1.00         1\n",
      "       20-20       1.00      1.00      1.00         3\n",
      "       28-28       1.00      1.00      1.00         3\n",
      "         DAP       1.00      1.00      1.00         4\n",
      "        Urea       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# from sklearn.metrics import accuracy_score, classification_report, r2_score\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # =====================\n",
    "# # 2. Preprocessing\n",
    "# # =====================\n",
    "# df = pd.read_csv(\"data/Fertilizer Prediction.csv\")\n",
    "\n",
    "# X = df.drop(\"Fertilizer Name\", axis=1)\n",
    "# y = df[\"Fertilizer Name\"]\n",
    "\n",
    "# # Encode categorical columns\n",
    "# for col in [\"Soil Type\", \"Crop Type\"]:\n",
    "#     X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "# # Encode target labels\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(y)\n",
    "\n",
    "# # Scale features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Train/val split\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# # =====================\n",
    "# # 3. Keras Neural Network\n",
    "# # =====================\n",
    "# print(\"\\n=== Keras Neural Network ===\")\n",
    "# model = keras.Sequential([\n",
    "#     layers.Dense(128, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Dense(64, activation=\"relu\"),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Dense(len(le.classes_), activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=20, batch_size=32, verbose=1\n",
    "# )\n",
    "\n",
    "# val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "# print(f\"✅ Keras Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# y_pred_nn = np.argmax(model.predict(X_val), axis=1)\n",
    "# print(\"\\nClassification Report (Keras NN):\")\n",
    "# print(classification_report(y_val, y_pred_nn, target_names=le.classes_))\n",
    "\n",
    "# # =====================\n",
    "# # 4. XGBoost Classifier\n",
    "# # =====================\n",
    "# print(\"\\n=== XGBoost Classifier ===\")\n",
    "# xgb = XGBClassifier(\n",
    "#     n_estimators=500,\n",
    "#     learning_rate=0.05,\n",
    "#     max_depth=6,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=42,\n",
    "#     objective=\"multi:softmax\",\n",
    "#     num_class=len(le.classes_),\n",
    "#     eval_metric=\"mlogloss\"\n",
    "# )\n",
    "\n",
    "# # Ensure y are integers\n",
    "# y_train = np.array(y_train, dtype=np.int32)\n",
    "# y_val = np.array(y_val, dtype=np.int32)\n",
    "\n",
    "# xgb.fit(X_train, y_train)\n",
    "\n",
    "# y_pred_xgb = xgb.predict(X_val)\n",
    "\n",
    "# print(\"✅ XGBoost Accuracy:\", accuracy_score(y_val, y_pred_xgb))\n",
    "# print(\"\\nClassification Report (XGBoost):\")\n",
    "# print(classification_report(y_val, y_pred_xgb, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8552d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌲 RandomForest Accuracy: 0.14325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    10-26-26       0.13      0.15      0.14      2876\n",
      "    14-35-14       0.15      0.16      0.15      2898\n",
      "    17-17-17       0.14      0.14      0.14      2834\n",
      "       20-20       0.15      0.14      0.15      2836\n",
      "       28-28       0.14      0.14      0.14      2847\n",
      "         DAP       0.15      0.14      0.14      2844\n",
      "        Urea       0.14      0.13      0.14      2865\n",
      "\n",
      "    accuracy                           0.14     20000\n",
      "   macro avg       0.14      0.14      0.14     20000\n",
      "weighted avg       0.14      0.14      0.14     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ishaan Kaul\\Desktop\\PROJECTS\\FarmSakha_AI\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Logistic Regression Accuracy: 0.1439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    10-26-26       0.14      0.14      0.14      2876\n",
      "    14-35-14       0.15      0.26      0.19      2898\n",
      "    17-17-17       0.15      0.07      0.10      2834\n",
      "       20-20       0.14      0.07      0.09      2836\n",
      "       28-28       0.14      0.14      0.14      2847\n",
      "         DAP       0.15      0.14      0.15      2844\n",
      "        Urea       0.14      0.18      0.16      2865\n",
      "\n",
      "    accuracy                           0.14     20000\n",
      "   macro avg       0.14      0.14      0.14     20000\n",
      "weighted avg       0.14      0.14      0.14     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ishaan Kaul\\Desktop\\PROJECTS\\FarmSakha_AI\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # Load your dataset\n",
    "# data = pd.read_csv(\"data/Fertilizer Prediction.csv\")\n",
    "\n",
    "# # One-hot encode categorical features\n",
    "# data = pd.get_dummies(data, columns=['Soil Type', 'Crop Type'])\n",
    "\n",
    "# # Split features & target\n",
    "# X = data.drop(\"Fertilizer Name\", axis=1)\n",
    "# y = data[\"Fertilizer Name\"]\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # RandomForest\n",
    "# rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "# rf_preds = rf.predict(X_val)\n",
    "\n",
    "# print(\"🌲 RandomForest Accuracy:\", accuracy_score(y_val, rf_preds))\n",
    "# print(classification_report(y_val, rf_preds))\n",
    "\n",
    "# # Logistic Regression (multinomial)\n",
    "# logreg = LogisticRegression(max_iter=500, multi_class=\"multinomial\")\n",
    "# logreg.fit(X_train, y_train)\n",
    "# log_preds = logreg.predict(X_val)\n",
    "\n",
    "# print(\"📊 Logistic Regression Accuracy:\", accuracy_score(y_val, log_preds))\n",
    "# print(classification_report(y_val, log_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b7547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
